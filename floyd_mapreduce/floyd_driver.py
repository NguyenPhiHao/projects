#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import subprocess
import sys

# ==================== C·∫§U H√åNH ====================
HADOOP_STREAMING = r"C:\hadoop-3.3.0\share\hadoop\tools\lib\hadoop-streaming-3.3.0.jar"
PYTHON = r"C:\Users\Admin\AppData\Local\Programs\Python\Python313\python.exe"
BASE_HDFS = "/floyd_warshall"
LOCAL_INPUT = r"C:\hadoop-3.3.0\projects\floyd_mapreduce\input_fw_undirected.txt"
LOCAL_WORKDIR = r"C:\hadoop-3.3.0\projects\floyd_mapreduce"
FW_LOCAL_RESULT = os.path.join(LOCAL_WORKDIR, "fw_result.txt")
# ==================================================

def run_cmd(cmd, check=False):
    print("üëâ", cmd)
    res = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if res.returncode != 0:
        print("‚ùå L·ªói:", res.stderr.strip())
        if check:
            sys.exit(1)
        return False
    return True

# ==================== T·∫†O INPUT ƒê·∫¶Y ƒê·ª¶ ====================
def get_nodes_from_input():
    """L·∫•y danh s√°ch c√°c ƒë·ªânh t·ª´ file input_fw_undirected.txt"""
    nodes = set()
    with open(LOCAL_INPUT, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                try:
                    u = int(parts[0]); v = int(parts[1])
                    nodes.add(u); nodes.add(v)
                except:
                    continue
    return sorted(nodes)

def make_full_input_temp(nodes):
    """Sinh ma tr·∫≠n ƒë·∫ßy ƒë·ªß (v√¥ h∆∞·ªõng)"""
    temp_file = os.path.join(LOCAL_WORKDIR, "input_full_auto.txt")
    edges = {}
    with open(LOCAL_INPUT, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) < 3:
                continue
            try:
                u, v, w = int(parts[0]), int(parts[1]), float(parts[2])
                edges[(u, v)] = w
                if (v, u) not in edges and u != v:
                    edges[(v, u)] = w
            except:
                continue

    with open(temp_file, 'w', encoding='utf-8') as f:
        for i in nodes:
            for j in nodes:
                if i == j:
                    f.write(f"{i} {j} 0.0\n")
                elif (i, j) in edges:
                    f.write(f"{i} {j} {edges[(i, j)]}\n")
                else:
                    f.write(f"{i} {j} INF\n")
    print(f"‚úÖ ƒê√£ sinh file ƒë·∫ßy ƒë·ªß (v√¥ h∆∞·ªõng): {temp_file}")
    return temp_file

def hdfs_put(full_input_path):
    run_cmd(f'hdfs dfs -rm -r -skipTrash "{BASE_HDFS}"')
    run_cmd(f'hdfs dfs -mkdir -p "{BASE_HDFS}/input"')
    run_cmd(f'hdfs dfs -put -f "{full_input_path}" "{BASE_HDFS}/input/input_full_auto.txt"', check=True)

# ==================== CH·∫†Y MAPREDUCE ====================
def run_iteration(k, input_hdfs, output_hdfs):
    mapper = f'"{PYTHON} floyd_mapper.py {k}"'
    reducer = f'"{PYTHON} floyd_reducer.py {k}"'
    files_list = (
        "file:///C:/hadoop-3.3.0/projects/floyd_mapreduce/floyd_mapper.py,"
        "file:///C:/hadoop-3.3.0/projects/floyd_mapreduce/floyd_reducer.py"
    )
    cmd = (
        f'hadoop jar "{HADOOP_STREAMING}" '
        f'-D mapreduce.job.maps=1 '       # th√™m d√≤ng n√†y
        f'-D mapreduce.job.reduces=1 '
        f'-files "{files_list}" '
        f'-mapper {mapper} -reducer {reducer} '
        f'-input "{input_hdfs}" -output "{output_hdfs}"'
    )
    return run_cmd(cmd)

def fetch_result_to_local(hdfs_output_dir, local_file):
    hdfs_part = f"{hdfs_output_dir}/part-00000"
    ok = run_cmd(f'hdfs dfs -test -e "{hdfs_part}"')
    if not ok:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y {hdfs_part}")
        return False
    cmd = f'hdfs dfs -cat "{hdfs_part}" > "{local_file}"'
    return run_cmd(cmd)

# ==================== CH∆Ø∆†NG TR√åNH CH√çNH ====================
def main():
    os.chdir(LOCAL_WORKDIR)
    nodes = get_nodes_from_input()
    if not nodes:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu input h·ª£p l·ªá.")
        return

    # 1Ô∏è‚É£ Chu·∫©n b·ªã input
    full_input_path = make_full_input_temp(nodes)
    hdfs_put(full_input_path)

    print(f"\n=== B·∫Øt ƒë·∫ßu Floyd‚ÄìWarshall (n = {len(nodes)}) ===")

    # 2Ô∏è‚É£ Ch·∫°y tu·∫ßn t·ª± theo k
    input_hdfs = f"{BASE_HDFS}/input/input_full_auto.txt"
    last_output = None

    for idx, k_label in enumerate(nodes, start=1):
        output_hdfs = f"{BASE_HDFS}/iter_{idx}"
        run_cmd(f'hdfs dfs -rm -r -skipTrash "{output_hdfs}"')
        print(f"\n=== V√≤ng k = {k_label} ===")
        ok = run_iteration(k_label, input_hdfs, output_hdfs)
        if not ok:
            print(f"‚ùå Job v√≤ng k={k_label} th·∫•t b·∫°i.")
            return
        last_output = output_hdfs
        input_hdfs = output_hdfs

    # 3Ô∏è‚É£ V√≤ng cu·ªëi (ƒë·ªìng b·ªô to√†n ƒë·ªì th·ªã)
    final_output = f"{BASE_HDFS}/iter_final"
    run_cmd(f'hdfs dfs -rm -r -skipTrash "{final_output}"')
    print("\n=== V√≤ng cu·ªëi (lan truy·ªÅn to√†n b·ªô ƒë∆∞·ªùng ƒëi) ===")
    if run_iteration(len(nodes) + 1, last_output, final_output):
        last_output = final_output

    # 4Ô∏è‚É£ K·∫øt qu·∫£ cu·ªëi
    if last_output:
        print("\n‚úÖ Ho√†n t·∫•t. T·∫£i k·∫øt qu·∫£ v·ªÅ local:", FW_LOCAL_RESULT)
        fetch_result_to_local(last_output, FW_LOCAL_RESULT)
    else:
        print("‚ùå Kh√¥ng c√≥ output cu·ªëi c√πng.")

if __name__ == "__main__":
    main()
